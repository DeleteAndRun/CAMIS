---
title: "R Template"
---

# XGBoost
XGBoost which stands for eXtreme Gradient Boosting is an efficent implementation of gradient boosting. Gradient boosting is an ensemble technique in machine learning. Unlike traditional models that learn from the data independently, boosting combines the predictions of multiple weak learners to create a single, more accurate strong learner. 

An XGBoost model is based on trees, so we don’t need to do much preprocessing for our data; we don’t need to worry about the factors or centering or scaling our data. 


## Available R packages

There are multiple packages that can be used to to implement xgboost in R.

-   [{tidymodels}](https://www.tidymodels.org/)
-   [{xgboost}](https://cran.r-project.org/web/packages/xgboost/index.html)
-   [{caret}](https://cran.r-project.org/web/packages/caret/index.html)

{tidymodels} and {caret} easy ways to access xgboost easily. This example will use {tidymodels} because of the functionality included in {tidymodels} and is being heavily supported by Posit.

## Data used

Data used for this example is `birthwt` which is part of the {MASS} package. This data-set considers a number of risk factors associated with birth weight in infants. 

```{r}
library(MASS)

head(birthwt)
```

Our modeling goal using the `birthwt` dataset is to predict whether the birth weight is low or not low based on factors such as mother's age, smoking status, and history of hypertension.

## Example Code
Use {tidymodels} metadata package to split the data into training and testing data.

```{r}
birthwt <- birthwt

set.seed(123)


brthwt_split <- initial_split(birthwt, strata = low)
brthwt_train <- training(brthwt_split)
brthwt_test <- testing(brthwt_split)

```

An XGBoost model is based on trees. 

### Example using {package name}

**Optional** if there is more than one package
